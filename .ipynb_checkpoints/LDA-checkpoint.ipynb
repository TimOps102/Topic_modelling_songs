{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188d247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf4f1d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0fe416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "frg_path= \"Songs/BRD-Charts/1_xml/\"\n",
    "gdr_path= \"Songs/DDR/1_xml/\"\n",
    "ndw_path= \"Songs/NDW/1_xml/\"\n",
    "\n",
    "xml_frg = [frg_path+f for f in os.listdir(frg_path) if f.endswith('.xml')]\n",
    "xml_gdr = [gdr_path+f for f in os.listdir(gdr_path) if f.endswith('.xml')]\n",
    "xml_ndw = [ndw_path+f for f in os.listdir(ndw_path) if f.endswith('.xml')]\n",
    "\n",
    "all_xml = [xml_frg,xml_gdr,xml_ndw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585c642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for category in all_xml:\n",
    "    for xml_file in category:\n",
    "        with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "            xml_data = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(xml_data, 'xml')\n",
    "        \n",
    "        cat = xml_file.split(\"/\")[1].split(\"-\")[0]\n",
    "        year = int(soup.find(\"date\").text)\n",
    "        title = soup.find(\"title\").text\n",
    "        author = soup.find(\"author\").text\n",
    "        text = \" \".join([k.text for k in soup.find(\"div1\", attrs={\"type\":\"song\"}).find_all(\"l\")])\n",
    "\n",
    "        data.append({'Category': cat, 'Year': year, 'Title': title, 'Author': author, 'Text': text})\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ad061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw_filter = (df[\"Category\"]==\"NDW\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "ndw_sub = df[ndw_filter].sample(n=202, random_state=42)\n",
    "brd_filter = (df[\"Category\"]==\"BRD\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "brd_sub = pd.concat([df[brd_filter],ndw_sub], ignore_index=True)\n",
    "ddr_filter = (df[\"Category\"]==\"DDR\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "df = pd.concat([df[ddr_filter],brd_sub], ignore_index=True)\n",
    "\n",
    "#we now rename the NDW category to BRD, because the belong to the BRD\n",
    "df['Category'] = df['Category'].replace('NDW', 'BRD')\n",
    "#the dataframe that remains, contains now out of 1000 Songs of the GDR from 1970 to 1990 and 1000 Songs from the FRG from 1970 to 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eef5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kennst', 'du', 'das', 'land', 'mit', 'seinen', 'alten', 'eichen', 'das', 'land', 'von', 'einstein', 'von', 'karl', 'marx', 'und', 'bach', 'wo', 'jede', 'antwort', 'endet', 'mit', 'dem', 'fragezeichen', 'wo', 'ich', 'ein', 'zimmer', 'habe', 'unterm', 'dach', 'wo', 'sich', 'so', 'viele', 'wegen', 'früher', 'oft', 'noch', 'schämen', 'wo', 'mancher', 'vater', 'eine', 'frage', 'nicht', 'versteht', 'wo', 'ihre', 'kinder', 'ihnen', 'das', 'nicht', 'übelnehmen', 'weil', 'seine', 'antwort', 'im', 'geschichtsbuch', 'steht', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'mit', 'seinen', 'seen', 'und', 'wäldern', 'das', 'kleine', 'land', 'das', 'man', 'an', 'einem', 'tag', 'durchfährt', 'wo', 'man', 'was', 'wird', 'auch', 'ohne', 'seine', 'eltern', 'doch', 'auch', 'beziehungen', 'sind', 'manchmal', 'etwas', 'wert', 'hier', 'steht', 'die', 'schule', 'und', 'mein', 'klassenzimmer', 'das', 'riecht', 'heut', 'immer', 'noch', 'nach', 'terpentin', 'von', 'mathe', 'hab', 'ich', 'heut', 'noch', 'keinen', 'schimmer', 'doch', 'vor', 'den', 'lehrern', 'kann', 'ich', 'meine', 'mütze', 'ziehn', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'wo', 'die', 'fabriken', 'uns', 'gehören', 'wo', 'der', 'prometheus', 'schon', 'um', 'fünf', 'aufsteht', 'hier', 'kann', 'man', 'manche', 'faust', 'auf', 'manchen', 'tischen', 'hören', 'bevor', 'dann', 'wieder', 'trotzdem', 'was', 'nicht', 'geht', 'wo', 'sich', 'auf', 'wohnungsämtern', 'hoffnungen', 'verlieren', 'wo', 'ein', 'parteitag', 'sich', 'darüber', 'sorgen', 'macht', 'wo', 'sich', 'die', 'leute', 'alles', 'selber', 'reparieren', 'weil', 'sie', 'das', 'werkzeug', 'haben', 'wissen', 'und', 'die', 'macht', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'mit', 'dem', 'problem', 'im', 'winter', 'das', 'züge', 'stoppt', 'und', 'an', 'die', 'fenster', 'klirrt', 'wo', 'wir', 'viel', 'reden', 'über', 'später', 'und', 'auch', 'kinder', 'und', 'wo', 'ein', 'cello', 'spielt', 'bevor', 'es', 'leise', 'wird', 'hier', 'lernte', 'meine', 'mutter', 'das', 'regieren', 'als', 'sie', 'vor', 'einem', 'trümmerhaufen', 'stand', 'ich', 'möchte', 'dieses', 'land', 'nie', 'mehr', 'verlieren', 'es', 'ist', 'mein', 'mutter', 'und', 'mein', 'vaterland']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        yield(gensim.utils.simple_preprocess(str(sentence)))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df[\"Text\"]))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b45cd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abends', 'wartest', 'du', 'ganz', 'zufällig', 'am', 'bus', 'du', 'weißt', 'genau', 'zu', 'welcher', 'zeit', 'ich', 'kommen', 'muss', 'du', 'legst', 'dein', 'lächeln', 'auf', 'und', 'tust', 'immer', 'vertraut', 'dass', 'jeder', 'denken', 'muss', 'ich', 'wäre', 'deine', 'braut', 'deine', 'braut', 'abends', 'kommst', 'du', 'an', 'und', 'lädst', 'dich', 'selber', 'ein', 'und', 'immer', 'mit', 'der', 'flasche', 'wein', 'sprichst', 'große', 'dinge', 'und', 'machst', 'es', 'dir', 'bequem', 'und', 'kannst', 'dann', 'gar', 'nicht', 'lachen', 'wenn', 'ich', 'dann', 'sage', 'du', 'sollst', 'geh', 'du', 'sollst', 'geh', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'vor', 'deinen', 'freunden', 'drehst', 'du', 'und', 'spielst', 'den', 'king', 'als', 'wär', 'ich', 'nur', 'ein', 'kleiner', 'fisch', 'der', 'an', 'dir', 'hing', 'du', 'kippst', 'die', 'sache', 'um', 'dass', 'du', 'dich', 'nicht', 'blamierst', 'ich', 'trinke', 'deinen', 'wein', 'und', 'merke', 'wie', 'du', 'frierst', 'wie', 'du', 'frierst', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'du', 'kommst', 'nicht', 'in', 'mein', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(bigram_mod[data_words[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a5e9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_.lower() for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6d00eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kennst', 'alt', 'eich', 'land', 'einstein', 'antwort', 'enden', 'fragezeichen', 'zimmer', 'dach', 'früh', 'oft', 'schämen', 'vater', 'frage', 'verstehen', 'kind', 'übelnehmen', 'antwort', 'geschichtsbuch', 'stehen', 'schaff', 'selber', 'geb', 'leben', 'sinn', 'teil', 'unsr', 'erde', 'selber', 'seen', 'klein', 'land', 'tag', 'durchfähren', 'eltern', 'beziehung', 'manchmal', 'wert', 'stehen', 'schule', 'klassenzimmer', 'riechen', 'heut', 'immer', 'terpentin', 'heut', 'schimm', 'lehrer', 'mütze', 'ziehn', 'schaff', 'selber', 'geb', 'leben', 'sinn', 'teil', 'unsr', 'erde', 'selber', 'land', 'fabrik', 'gehören', 'schon', 'aufstehen', 'tisch', 'hören', 'trotzdem', 'gehen', 'wohnungsämtern', 'hoffnung', 'verlieren', 'parteitag', 'darüber', 'sorgen', 'machen', 'leute', 'selber', 'reparieren', 'werkzeug', 'wissen', 'macht', 'schaff', 'selber', 'geb', 'leben', 'sinn', 'teil', 'unsr', 'erde', 'selber', 'land', 'problem', 'züge', 'stoppen', 'fenster', 'klirren', 'reden', 'spät', 'kind', 'spielen', 'bevor', 'leise', 'lernt', 'mutter', 'regieren', 'trümmerhaufe', 'stehen', 'land', 'nie', 'mehr', 'verlieren', 'mutter']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('de_core_news_md', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dcf7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 3), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 3), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 1), (30, 1), (31, 5), (32, 3), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 3), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 7), (58, 3), (59, 1), (60, 1), (61, 1), (62, 3), (63, 1), (64, 1), (65, 3), (66, 1), (67, 1), (68, 1), (69, 1), (70, 3), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2790d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 13788\n",
      "Total Vocabulary Size: 2745\n"
     ]
    }
   ],
   "source": [
    "print('Total Vocabulary Size:', len(id2word))\n",
    "id2word.filter_extremes(no_below=4, no_above=0.95)\n",
    "print('Total Vocabulary Size:', len(id2word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "songs",
   "language": "python",
   "name": "songs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c191342c",
   "metadata": {},
   "source": [
    "# Keyword Extraction\n",
    "\n",
    "The simple count based method to extract sublanguage specific vocabulary only allows explorative approaches. It gives no objective measurement of how specific a word is to a sublanguage corpus.\n",
    "To alleviate this problem we can either use Log-Likelihood or tf-idf to extract sublanguage specific vocabulary.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "To determine the difference between 2 or more sources, we have to formulate a weight for the\n",
    "each word with regards to each text source. One possible measure is the tf/idf measure which is a weighting bases on the unique usage of a term in single documents. The more often a term is used in different\n",
    "documents the less importance it gets w.r.t. the tf/idf weight. In detail, this follows the intuition\n",
    "that a term which appears very often can’t be unique to a certain class or domain.\n",
    "Following Wikipedia, the tf-idf value increases proportionally to the number of times a word\n",
    "appears in the document, but is often offset by the frequency of the word in the corpus, which\n",
    "helps to adjust for the fact that some words appear more frequently in general.[1]\n",
    "\n",
    "For normalized term frequency $tf(t,D)$ there are various options (see lecture, videos in moodle or research).\n",
    "\n",
    "### Log Likelihood \n",
    "Another possibility to measure relative importance of terms is Log-Likelihood.\n",
    "When using a reference corpus for comparison we use the term counts in the different domains and\n",
    "a reference corpus in order to determine significant differences. \n",
    "The used significance test is called the “Log-Likelihood”-Ratio Test (LL). The LL-value gives the expectation of a term to be appearing in the target w.r.t. the reference\n",
    "corpus. \n",
    "\n",
    "\n",
    "### Copora\n",
    "\n",
    "We provide text for the three domains `Automobil`, `Wirtschaft` and `Sport`.\n",
    "When in need of a reference corpus go to the wortschatz-portal and download a large enough sample of references, around 1 million sentences should suffice.\n",
    "\n",
    "### Text Preprocessing\n",
    "\n",
    "Be aware that the prepocessing of text has considerable influence on the outcome. Part of this exercise is to\n",
    "to deploy a reasonable preprocessing pipeline. Make use of the knowledge about the Zipf distribution and other text preprocessing techniques.\n",
    "\n",
    "To analyze differences we need to build a single \"document\" for each domain. This\n",
    "means, if there are more than one document per domain, we’ll concat all texts belonging to one domain to a single text source.\n",
    "\n",
    "### Also:\n",
    "\n",
    "It makes sense to first introduce a function that transforms a collection of documents into an Document-Term-Matrix (DTM). For that, the numpy library's array class is worth a look. Also data sizes may exceed memory. It is important to consider data structures to accomodate for that, e.g. sparse arrays.\n",
    "\n",
    "**Hint 1** If you use numpy, be aware that numpy contains a lot of useful functions like logarithms or sorting.\n",
    "\n",
    "**Hint 2** Beware of numerical traps like the undefined logarithm of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3438204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2126ce7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['keyword/automobil_50k.txt', 'keyword/wirtschaft_50k.txt', 'keyword/sport_50k.txt'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "content = {}\n",
    "with zipfile.ZipFile(\"keyword.zip\") as zfile:\n",
    "    for f in zfile.namelist():\n",
    "        if f != \"keyword/\":\n",
    "            content[f] = zfile.read(f).decode(\"utf8\")\n",
    "content.keys()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1a6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../news_de.zip\"\n",
    "def load_zip(path):\n",
    "    import zipfile\n",
    "    r = {}\n",
    "    with zipfile.ZipFile(path, \"r\") as zfile:\n",
    "        for file in zfile.namelist():\n",
    "            with zfile.open(file, \"r\") as f:\n",
    "                r[file.split(\"_\")[2]] = \" \".join([x.decode(\"utf8\").split(\"\\t\")[1].lower() for x in f.readlines()])\n",
    "    return r\n",
    "reference = \" \".join(list(load_zip(path).values())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5583b3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'um die in jeder hinsicht zufriedenzustellen, tüftelt er einen weg aus, sinnlose bürokratie wie laden'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    nlp = spacy.load('de_core_news_md')\n",
    "    txt = txt if len(txt[0]) > 1 else nltk.word_tokenize(txt)\n",
    "    txt = [x.replace(\"ß\",\"ss\").lower() for x in txt if x.isalpha() and len(x) > 1]\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c85a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def create_dtm(self, texts, cut_first=200, min_freq=3):\n",
    "        \n",
    "        # Clean texts\n",
    "        self.texts = {k:preprocess(v) for k,v in texts.items()}\n",
    "        \n",
    "        # Count texts\n",
    "        self.vocab = nltk.FreqDist(sum(self.texts.values(),[]))\n",
    "        \n",
    "        # Prune overall vocabulary\n",
    "        self.vocab = sorted(list(self.vocab.items()),key= lambda x: -x[1]) \n",
    "        self.vocab = [x[0] for x in self.vocab[200:] if x[1] >=min_freq]\n",
    "        \n",
    "        # count and restrict domain level text to the vocabulary\n",
    "        self.term_frequencies = {genre: nltk.FreqDist(text) for genre, text in self.texts.items()}\n",
    "        self.dtm = np.array([[v.get(w,0) for w in self.vocab] for k,v in self.term_frequencies.items()])\n",
    "    \n",
    "    def tfidf(self):\n",
    "        tf = np.log(self.dtm + 1e-25)\n",
    "        #tf = 0.5 + 0.5 * self.dtm / self.dtm.max(-1, keepdims=True) # alternative normalization.\n",
    "        idf = np.log(self.dtm.shape[0] /((da.dtm>0).sum(0) + 1e-25))\n",
    "        return tf * idf\n",
    "\n",
    "    def tfidf_keywords(self, n=10):\n",
    "        \"\"\"Iterate all copora for printing\"\"\"\n",
    "        tfidf = self.tfidf()\n",
    "        return {k:[self.vocab[k] for k in tfidf[i].argsort(0)[::-1][:n]] for i,k in enumerate(self.texts.keys())}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7838982",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = TFIDF()\n",
    "da.create_dtm(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296fe2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword/automobil_50k.txt': ['hubraum',\n",
       "  'serienmässig',\n",
       "  'motorhaube',\n",
       "  'kühlergrill',\n",
       "  'lamborghini',\n",
       "  'durchschnittsverbrauch',\n",
       "  'mittelkonsole',\n",
       "  'rückbank',\n",
       "  'fahrleistungen',\n",
       "  'gti',\n",
       "  'concept',\n",
       "  'heckklappe',\n",
       "  'selbstzünder',\n",
       "  'zweisitzer',\n",
       "  'supersportwagen',\n",
       "  'touran',\n",
       "  'vierzylinder',\n",
       "  'zylinder',\n",
       "  'sechszylinder',\n",
       "  'cdi',\n",
       "  'hinterachse',\n",
       "  'motorshow',\n",
       "  'newtonmeter',\n",
       "  'facelift',\n",
       "  'cabrios'],\n",
       " 'keyword/wirtschaft_50k.txt': ['dax',\n",
       "  'gdl',\n",
       "  'zentralbank',\n",
       "  'commerzbank',\n",
       "  'iwf',\n",
       "  'mehdorn',\n",
       "  'arbeitsmarkt',\n",
       "  'karstadt',\n",
       "  'arcandor',\n",
       "  'microsoft',\n",
       "  'aktienmarkt',\n",
       "  'währungsfonds',\n",
       "  'hre',\n",
       "  'finanzmärkten',\n",
       "  'verdi',\n",
       "  'lehman',\n",
       "  'postbank',\n",
       "  'notenbank',\n",
       "  'grossbank',\n",
       "  'tecdax',\n",
       "  'zumwinkel',\n",
       "  'mdax',\n",
       "  'staatsanleihen',\n",
       "  'grossbanken',\n",
       "  'bip'],\n",
       " 'keyword/sport_50k.txt': ['tsv',\n",
       "  'vfb',\n",
       "  'hertha',\n",
       "  'sg',\n",
       "  'dfb',\n",
       "  'borussia',\n",
       "  'torhüter',\n",
       "  'tabellenführer',\n",
       "  'bsc',\n",
       "  'fsv',\n",
       "  'gaal',\n",
       "  'hoffenheim',\n",
       "  'durchgang',\n",
       "  'nowitzki',\n",
       "  'torwart',\n",
       "  'magath',\n",
       "  'timo',\n",
       "  'freistoss',\n",
       "  'rekordmeister',\n",
       "  'inter',\n",
       "  'ribéry',\n",
       "  'treffern',\n",
       "  'fv',\n",
       "  'achtelfinale',\n",
       "  'rückrunde']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.tfidf_keywords(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08e269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLike:\n",
    "    def create_dtm(self, texts, cut_first=200, min_freq=3):\n",
    "\n",
    "        # Clean texts\n",
    "        self.texts = {k: preprocess(v) for k, v in texts.items()}\n",
    "\n",
    "        # Count texts\n",
    "        self.term_frequencies = {genre: nltk.FreqDist(text) for genre, text in self.texts.items()}\n",
    "        self.vocab = sum(self.term_frequencies.values(), nltk.FreqDist())\n",
    "\n",
    "        # Prune overall vocabulary\n",
    "        self.vocab = sorted(list(self.vocab.items()), key=lambda x: -x[1])\n",
    "        self.vocab = [x[0] for x in self.vocab[200:] if x[1] >= min_freq]\n",
    "        self.term_frequencies = {genre: {k:freq[k] for k in (set(freq.keys()) & set(self.vocab)) }  for genre, freq in self.term_frequencies.items()}\n",
    "\n",
    "\n",
    "        # count and restrict domain level text to the vocabulary\n",
    "        self.dtm = np.array([[v.get(w, 0) for w in self.vocab] for k, v in self.term_frequencies.items()])\n",
    "\n",
    "    def log_likelihood(self, corpus, n=None, threshold=None):\n",
    "        i = list(da.texts.keys()).index(corpus)\n",
    "       \n",
    "        a = self.dtm[i]\n",
    "        b = self.dtm[list(da.texts.keys()).index(\"reference\")]\n",
    "\n",
    "\n",
    "        c = a.sum()\n",
    "        d = b.sum()\n",
    "\n",
    "        e1 = c * (a + b) / (c + d) + 1e-25\n",
    "        e2 = d * (a + b) / (c + d) + 1e-25\n",
    "\n",
    "        ll = 2 * (a * np.log(a / e1 + 1e-25) + b * np.log(b / e2 + 1e-25))\n",
    "\n",
    "        if threshold is not None:\n",
    "            return [da.vocab[k.item()] for k in np.where(ll > threshold)[0]]\n",
    "\n",
    "        if n is not None:\n",
    "            return [self.vocab[k] for k in ll.argsort(0)[::-1][:n]]\n",
    "\n",
    "\n",
    "    def ll_keywords(self, n=None, threshold=None):\n",
    "        return {k: self.log_likelihood(k, n=n, threshold=threshold) for k in self.texts.keys() if k != \"reference\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eff41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[\"reference\"] = reference #Add the reference corpus\n",
    "da = LogLike()\n",
    "da.create_dtm(content,cut_first=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a0489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword/automobil_50k.txt': ['liter',\n",
       "  'bmw',\n",
       "  'audi',\n",
       "  'vw',\n",
       "  'wagen',\n",
       "  'mercedes',\n",
       "  'fahrzeuge',\n",
       "  'dm',\n",
       "  'porsche',\n",
       "  'toyota',\n",
       "  'hersteller',\n",
       "  'diesel',\n",
       "  'motor',\n",
       "  'opel',\n",
       "  'litern',\n",
       "  'verbrauch',\n",
       "  'modell',\n",
       "  'modelle',\n",
       "  'marke',\n",
       "  'coupé',\n",
       "  'ford',\n",
       "  'fahrer',\n",
       "  'fahren',\n",
       "  'fahrzeug',\n",
       "  'gestern'],\n",
       " 'keyword/wirtschaft_50k.txt': ['dm',\n",
       "  'bank',\n",
       "  'dollar',\n",
       "  'banken',\n",
       "  'krise',\n",
       "  'opel',\n",
       "  'quartal',\n",
       "  'verlinken',\n",
       "  'konzern',\n",
       "  'link',\n",
       "  'gm',\n",
       "  'kostenfrei',\n",
       "  'ubs',\n",
       "  'finanzkrise',\n",
       "  'kunden',\n",
       "  'porsche',\n",
       "  'artikel',\n",
       "  'verwenden',\n",
       "  'mrd',\n",
       "  'bahn',\n",
       "  'unten',\n",
       "  'franken',\n",
       "  'stehenden',\n",
       "  'möchten',\n",
       "  'polizei'],\n",
       " 'keyword/sport_50k.txt': ['trainer',\n",
       "  'saison',\n",
       "  'mannschaft',\n",
       "  'dm',\n",
       "  'spieler',\n",
       "  'sieg',\n",
       "  'sv',\n",
       "  'team',\n",
       "  'minute',\n",
       "  'wm',\n",
       "  'verlinken',\n",
       "  'link',\n",
       "  'tor',\n",
       "  'league',\n",
       "  'kostenfrei',\n",
       "  'bayern',\n",
       "  'löw',\n",
       "  'spd',\n",
       "  'minuten',\n",
       "  'spielen',\n",
       "  'partie',\n",
       "  'stehenden',\n",
       "  'verwenden',\n",
       "  'artikel',\n",
       "  'liga']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llk = da.ll_keywords(n=25)\n",
    "llk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad59de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad9cb8d",
   "metadata": {},
   "source": [
    "# Cooccurrences\n",
    "\n",
    "Cooccurrences are word forms that appear significantly frequently together. In other words: If one of the words in a cooccurring pair appears in a sentence it is very likely to also find the other one.\n",
    "\n",
    "Watch the additional material on cooccurrences. \n",
    "\n",
    "Your have the following corpus of seven sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57f946",
   "metadata": {},
   "source": [
    "```\n",
    "\"The dog plays.\",\n",
    "\"The child plays.\",\n",
    "\"The moon and the sun are shining.\",\n",
    "\"The sun is burning\",\n",
    "\"The fire is burning\",\n",
    "\"The moon is shining\",\n",
    "\"The candle is burning and shining\",\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57e9ab",
   "metadata": {},
   "source": [
    "### Document Term Matrix and Term-Term Matrix\n",
    "1) Use this to build a document term matrix and then a term-term-matrix. You can consider `all`, `the`, `with`, `and`, `are`,  and `is`  stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9614b",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Document-Term-Matrix\n",
    "\n",
    "|D|   dog  | plays | child    |  moon  |  sun | shining | burning |  fire |  candle| \n",
    "|-|--------|-------|----------|--------|------|---------|---------|-------|--------|\n",
    "|1|   $1$  |  $1$  |   $0$    |  $0$   |  $0$ |   $0$   |   $0$   |  $0$  |   $0$  |  \n",
    "|2|   $0$  |  $1$  |   $1$    |  $0$   |  $0$ |   $0$   |   $0$   |  $0$  |   $0$  |  \n",
    "|3|   $0$  |  $0$  |   $0$    |  $1$   |  $1$ |   $1$   |   $0$   |  $0$  |   $0$  |  \n",
    "|4|   $0$  |  $0$  |   $0$    |  $0$   |  $1$ |   $0$   |   $1$   |  $0$  |   $0$  |  \n",
    "|5|   $0$  |  $0$  |   $0$    |  $0$   |  $0$ |   $0$   |   $1$   |  $1$  |   $0$  |  \n",
    "|6|   $0$  |  $0$  |   $0$    |  $1$   |  $0$ |   $1$   |   $0$   |  $0$  |   $0$  |  \n",
    "|7|   $0$  |  $0$  |   $0$    |  $0$   |  $0$ |   $1$   |   $1$   |  $0$  |   $1$  |  \n",
    "\n",
    "Term-Term Matrix\n",
    "\n",
    "\n",
    "|   .     |dog  |plays| child | moon  |  sun | shining | burning |  fire |  candle|  \n",
    "|---------|-----|-----|-------|-------|------|---------|---------|-------|--------|\n",
    "  dog     |1    |  1  |    0  |    0  |   0  |    0    |    0    |    0  |    0  |   \n",
    "  plays   |1    |  2  |    1  |    0  |   0  |    0    |    0    |    0  |    0  |   \n",
    "child|0   |1    |  1  |    0  |    0  |   0  |    0    |    0    |    0  |   \n",
    "    moon  |0    |  0  |    0  |    2  |   1  |    2    |    0    |    0  |    0  |   \n",
    " sun      |0    |  0  |    0  |    1  |   2  |    1    |    1    |    0  |    0  |   \n",
    " shining  |0    |  0  |    0  |    2  |   1  |    3    |    1    |    0  |    1  |   \n",
    " burning  |0    |  0  |    0  |    0  |   1  |    1    |    3    |    1  |    1  |   \n",
    " fire     |0    |  0  |    0  |    0  |   0  |    0    |    1    |    1  |    0  |   \n",
    " candle   |0    |  0  |    0  |    0  |   0  |    1    |    1    |    0  |    1  |\n",
    " \n",
    " \n",
    "This is the full solution, usually the diagonal will be set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918be6c",
   "metadata": {},
   "source": [
    "### Cooccurrences\n",
    "2) Calculate the DICE-coefficient for (`sun` and `shining`)  and for (`moon` and `shining`) and (`sun` and `moon`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138614e",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "For (sun, shining)<br>\n",
    "ki = 2<br>\n",
    "kj = 3<br>\n",
    "kij = 1<br>\n",
    "dice = 2/5<br>\n",
    "\n",
    "For (moon, shining)<br>\n",
    "ki = 2<br>\n",
    "kj = 3<br>\n",
    "kij = 2<br>\n",
    "dice = 4/5<br>\n",
    "\n",
    "For (moon, sun)<br>\n",
    "ki = 2<br>\n",
    "kj = 2<br>\n",
    "kij = 1<br>\n",
    "dice = 2/4<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c931a9",
   "metadata": {},
   "source": [
    "3) We can use the Term-Term matrix and the cosine similarity to determine a semantic distance of two words. Each word is represented as the vector of its cooccurring words. calculate the semantic similarity of `sun`, `moon` and `shining`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a54d88",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\n",
    "cos(sun, moon) = 0.756\n",
    "\n",
    "cos(sun, shining) = 0.917\n",
    "\n",
    "cos(moon, shining) = 0.756\n",
    "\n",
    "\n",
    "**Setting the diagonal to zero we get:**\n",
    "\n",
    "cos(sun, moon) = 0.516\n",
    "\n",
    "cos(sun, shining) = 0.16\n",
    "\n",
    "cos(moon, shining) = 0.655\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03666288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 0 0 0]\n",
      " [1 2 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 2 1 2 0 0 0]\n",
      " [0 0 0 1 2 1 1 0 0]\n",
      " [0 0 0 2 1 3 1 0 1]\n",
      " [0 0 0 0 1 1 3 1 1]\n",
      " [0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6546536707079772"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dtm = np.array([[1,1,0,0,0,0,0,0,0],\n",
    "[0,1,1,0,0,0,0,0,0 ],\n",
    "[0,0,0,1,1,1,0,0,0] ,\n",
    "[0,0,0,0,1,0,1,0,0] ,\n",
    "[0,0,0,0,0,0,1,1,0] ,\n",
    "[0,0,0,1,0,1,0,0,0] ,\n",
    "[0,0,0,0,0,1,1,0,1]] )\n",
    "dtm.shape\n",
    "tt = dtm.T @ dtm\n",
    "print(tt)\n",
    "for i in range(tt.shape[0]):\n",
    "    tt[i,i] =0\n",
    "\n",
    "i = 4\n",
    "j = 5\n",
    "(tt[i]/np.linalg.norm(tt[i])).T @ (tt[j]/np.linalg.norm(tt[j]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2bc31a",
   "metadata": {},
   "source": [
    "4) The following top 14 words occurring in the same sentence as the words \"Forschende\", \"Wissenschaftlerin\", \"Wissenschaflter\" and \"Politikerin\" following the Wortschatz Portal are:\n",
    "\n",
    "```\n",
    "Forschende: haben (707), Universität (686), ETH (555), Zürich (372), Universität Bern (350), herausgefunden (327), ETH Zürich (314), Bern (276), Fachmagazin (208), Uni (201), Studierende (200), berichten (178), Max-Planck-Institut (167), Universität Zürich (167),\n",
    "\n",
    "Wissenschaftlerin: die (364), Universität (217), sagt (137), Die (132), leitende (127), erklärt (123), Priesemann (121), forscht (118), eine (116), als (99), Thi (93), Viola Priesemann (92), Hilde Mangold (91), Max-Planck-Institut (88),\n",
    "\n",
    "Wissenschaftler: Wissenschaftlerinnen (6,130), haben (3,945), Universität (3,263), Studie (3,008), die (2,996), dass (2,174), forschen (1,777), berichten (1,562), herausgefunden (1,545), untersuchten (1,490), Politiker (1,474), der (1,461), untersuchen (1,446), University (1,416)\n",
    "\n",
    "\n",
    "Politikerin: die (1,185), Die (510), parteilose (507), sagte (482), eine (375), als (368), sie (367), beliebteste (333), Partei (300), Politiker (293), konservative (280), grüne (237), sozialistische (219), Wagenknecht (217), \n",
    "```\n",
    "\n",
    "Calculate the semantic distance between these words using the cosine distance. Explain what you see.\n",
    "\n",
    "**Note**: Multi word units are considered one entry in the vocabulary, e.g. \"ETH Zürich\" is one unit and is different from \"ETH\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1208",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We want to use the vector of cooccurring words to represent the targets. Since the metric will be cosine distance every position that occurrs only once will have no impact on the result. So to save the work, we can restrict the vocabulary to words that occur in at least 2 target words.\n",
    "This leaves us with:\n",
    "\n",
    "vocab = 'berichten', 'als', 'Universität', 'Politiker', 'eine', 'Die', 'herausgefunden', 'Max-Planck-Institut', 'die'\n",
    "\n",
    "We then sort the number of occurrences into a vector such that the order of words for all targets is the same:\n",
    "\n",
    "Forschende        = (0, 208,   0,   0,   0,   0,   0, 707, 555,   0, 327,   0, 178,\n",
    "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 686,   0,   0,\n",
    "         0, 167,   0, 372,   0,   0, 314,   0,   0, 276,   0, 167,   0,\n",
    "         0, 201, 350,   0, 200)<br>\n",
    "wissenschaftlerin = ( 91,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "       121, 364, 132, 127,   0,   0,   0,   0,   0,   0, 217,  93, 116,\n",
    "        99,   0, 118,   0,   0,   0,   0,   0, 123,   0,   0,  88,   0,\n",
    "        92,   0,   0, 137,   0)<br>\n",
    "Wissenschaftler   = (91,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
    "       121, 364, 132, 127,   0,   0,   0,   0,   0,   0, 217,  93, 116,\n",
    "        99,   0, 118,   0,   0,   0,   0,   0, 123,   0,   0,  88,   0,\n",
    "        92,   0,   0, 137,   0)<br>\n",
    "Politikerin       = (  0,    0,  482,  217,  293,  367,  333,    0,    0,    0,    0,\n",
    "          0,    0,    0, 1185,  510,    0,  507,    0,    0,  219,    0,\n",
    "          0,    0,    0,  375,  368,    0,    0,    0,  300,    0,    0,\n",
    "        237,    0,    0,    0,    0,  280,    0,    0,    0,    0,    0) <br>\n",
    "\n",
    "\n",
    "\n",
    "No we calculate the cosine distance:\n",
    "\n",
    "cos(Forschende, Wissenschaftlerin) = 0.200<br>\n",
    "cos(Forschende, Wissenschaftler) = 0.398\n",
    "\n",
    "cos(Wissenschaftlerin, Wissenschaftler) = 0.304\n",
    "\n",
    "cos(Forschende, Politikerin) = 0<br>\n",
    "cos(Wissenschaftlerin, Politikerin) = 0.572<br>\n",
    "cos(Wissenschaftler, Politikerin) = 0.221\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a12b09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hilde Mangold', 'Fachmagazin', 'sagte', 'Wagenknecht', 'Politiker', 'sie', 'beliebteste', 'haben', 'ETH', 'Studie', 'herausgefunden', 'untersuchen', 'berichten', 'Priesemann', 'die', 'Die', 'leitende', 'parteilose', 'Wissenschaftlerinnen', 'dass', 'sozialistische', 'untersuchten', 'University', 'Universität', 'Thi', 'eine', 'als', 'Universität Zürich', 'forscht', 'Zürich', 'Partei', 'der', 'ETH Zürich', 'grüne', 'erklärt', 'Bern', 'forschen', 'Max-Planck-Institut', 'konservative', 'Viola Priesemann', 'Uni', 'Universität Bern', 'sagt', 'Studierende']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19912420989569674"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forschende= {\"haben\": 707, \"Universität\": 686, \"ETH\": 555, \"Zürich\": 372, \"Universität Bern\":350, \"herausgefunden\":327, \"ETH Zürich\":314, \"Bern\":276, \"Fachmagazin\":208, \"Uni\":201, \"Studierende\":200, \"berichten\":178, \"Max-Planck-Institut\": 167, \"Universität Zürich\":167}\n",
    "Wissenschaftlerin = {\"die\": 364, \"Universität\": 217, \"sagt\":137, \"Die\":132, \"leitende\":127, \"erklärt\": 123, \"Priesemann\": 121, \"forscht\": 118, \"eine\": 116, \"als\":99, \"Thi\": 93, \"Viola Priesemann\": 92, \"Hilde Mangold\":91, \"Max-Planck-Institut\":88}\n",
    "Wissenschaftler ={\"Wissenschaftlerinnen\": 6130, \"haben\": 3945, \"Universität\": 3263, \"Studie\":3008, \"die\":2996, \"dass\":2174, \"forschen\":1777, \"berichten\":1562, \"herausgefunden\":1545, \"untersuchten\":1490, \"Politiker\":1474, \"der\":1461, \"untersuchen\":1446, \"University\": 1416}\n",
    "Politikerin= {\"die\": 1185, \"Die\": 510, \"parteilose\":507, \"sagte\": 482, \"eine\": 375, \"als\":368, \"sie\": 367, \"beliebteste\": 333, \"Partei\":300, \"Politiker\": 293, \"konservative\": 280, \"grüne\":237, \"sozialistische\":219, \"Wagenknecht\":217}\n",
    "\n",
    "\n",
    "vocab = set(list(Forschende.keys()) + list(Wissenschaftler.keys())\n",
    "            + list(Wissenschaftlerin.keys()) + list(Politikerin.keys()))\n",
    "\n",
    "print(list(vocab))\n",
    "Forschende= np.array([Forschende.get(x,0) for x in vocab])\n",
    "Wissenschaftlerin= np.array([Wissenschaftlerin.get(x,0) for x in vocab])\n",
    "Wissenschaftler= np.array([Wissenschaftler.get(x,0) for x in vocab])\n",
    "Politikerin= np.array([Politikerin.get(x,0) for x in vocab])\n",
    "\n",
    "Politikerin\n",
    "\n",
    "i = Wissenschaftlerin\n",
    "j = Forschende\n",
    "(i/np.linalg.norm(i)).T @ (j/np.linalg.norm(j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

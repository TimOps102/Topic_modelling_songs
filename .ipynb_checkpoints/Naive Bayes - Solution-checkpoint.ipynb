{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb16380",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e54eea",
   "metadata": {},
   "source": [
    "# A1 Transmitter \n",
    "A \"1\" is observed at the local end of an error-prone binary communication channel Y. The source at the far end of the channel is known \"a priori\" to transmit ones with probability p(X=1)=0.20. The channel is defined by the transmission matrix of conditional probabilities \n",
    "\n",
    "$$\n",
    "p(Y=1|X=1)=0.90 \\\\ p(Y=1|X=0)=0.25\\\\\n",
    "p(Y=0|X=1)=0.10 \\\\ p(Y=0|X=0)=0.75\n",
    "$$\n",
    "\n",
    "\n",
    "completely characterized. What symbol X*=? did the source most likely send? Calculate using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a3e83",
   "metadata": {},
   "source": [
    "# Lösungsweg:\n",
    "$X=0:\\quad     p(X=0|Y=1) = p(Y=1|X=0)\\cdot p(X=0)\\cdot c = 0.25 \\cdot  0.8 \\cdot  c = 0.2\\cdot  c$\n",
    "\n",
    "\n",
    "$X=1:\\quad    p(X=1|Y=1) = p(Y=1|X=1)\\cdot p(X=1)\\cdot c = 0.9 \\cdot  0.2 \\cdot  c = 0.9 \\cdot  0.2 \\cdot  c$\n",
    "\n",
    "Solution: X* = 0 because p(X=0|Y=1) die maximale Wk. ist.\n",
    "\n",
    "Note: constant $c = 1/p(Y=1)$ and is the same in both cases, which is why we can neglect its exact value. You also can marginalize over X to get $P(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9975e",
   "metadata": {},
   "source": [
    "# A2 - Sentence Segmenter: \n",
    "An unknown (sentence) segmenter operating on heuristic principles considers a corpus as a sequence of word forms. A point is interpreted as a sentence boundary for segmentation, depending on several things:\n",
    "\n",
    "    - Whether the preceding word is an abbreviation,\n",
    "    - if so, whether it is always terminated by a dot, and whether it is clustered at the end of the sentence\n",
    "    - how long the previous segment is, etc. ...\n",
    "    \n",
    "It is known \"a priori\" that the segmenter S works correctly in 99% of all cases, incorrectly otherwise. After segmentation, the corpus is classified by language. The correctness k of the classification result C would depend on a correct sentence segmentation:\n",
    "\n",
    "$p(C=k|S=k)=0.80 $<br>$p(C=k|S=f)=0.30$\n",
    "\n",
    "$p(C=f|S=k)=0.20 $<br>$p(C=f|S=f)=0.70$\n",
    "\n",
    "We observe a corpus that has been assigned an incorrect language, but we know that overall the classifier is wrong only 14% of the time: p(C=f)=0.14. What is the probability that incorrect segmentation has occurred? (Hint: Bayes' theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cd837",
   "metadata": {},
   "source": [
    "### Lösung:\n",
    "$p(S=f|C=f) = \\frac{p(C=f|S=f)\\cdot p(S=f)}{p(C=f)} =  .70 \\cdot .01 / .14  = 70/14 /100 = 10/2 /100 = 5\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e95f2",
   "metadata": {},
   "source": [
    "# A4 - Sniffer Dog\n",
    "The sniffer dog problem: A customs dog should bark whenever it smells a passenger transporting drugs. Let $B$ be the event \"dog barks\" and $D$ be the event \"passenger is carrying drugs.\" The following probabilities are given: $P(B \\mid D) = 0.98$, $P( B \\mid \\overline D) = 0.03$, $P(D) = 0.01$.\n",
    "\n",
    "Determine the probability that a passenger is really carrying drugs if the dog barks, and the probability that the passenger is not carrying drugs if the dog does not bark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e3421",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We are looking for $P(D\\mid B)$ and $P(\\overline D\\mid \\overline B)$. From the given probabilities:\n",
    "\n",
    "$P(\\overline B\\mid D)=0.02$, $P(\\overline B\\mid \\overline D)=0.97$ and $P(\\overline D)=0.99$. Using Bayes' law:\n",
    "$$ \\begin{array}{c@{\\ =\\ }c@{\\ =\\ }c}\n",
    "P(D\\mid B) &= \\displaystyle \\frac{P(D) P(B\\mid D)}{P(D) P(B\\mid D)+P(\\overline D) P(B\\mid \\overline D)} &= 0.248 \\\\[4ex]\n",
    "P(\\overline D\\mid \\overline B) &= \\displaystyle \\frac{P(\\overline D) P(\\overline B\\mid \\overline D)}{P(\\overline D) P(\\overline B\\mid \\overline D)+P(D) P(\\overline B\\mid D)} &= 1.000\n",
    "\\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd346b",
   "metadata": {},
   "source": [
    "### Language Detection\n",
    "\n",
    "You can extend the simple count based language detection module from an earlier exercise to use Naive Bayes Principles. Instead of only looking for hits in the top ngrams of a language, we can consider probabilities of ngrams. \n",
    "\n",
    "For simplicity (naivety), we assume that all ngram probabilities only depend on the language, but are otherwise independent of each other.\n",
    "\n",
    "For any ngram $x_i$ and language $L_k$ the model calculates the probability distributions $p(x_i | L_k)$ and $p(x_i)$. The final evaluation if a class is matching a certain document is made by\n",
    "$$ \\hat{y} = \\text{argmax}_{k∈{1,...,K}} p(L_k)  \\prod^n_{i=1} p(x_i| L_k) $$\n",
    "\n",
    "\n",
    "We will also use “Add one” or “Laplacian” Smoothing. For the observed ngrams a pseudo count is\n",
    "introduced and a virtual count of 1 is added.\n",
    "\n",
    "\n",
    " $$p(x_i| L_k) = \\frac{count(x_i| L_k) + 1}{ count(x_k) + |V| + 1}$$\n",
    "\n",
    "For unknown ngrams in the documents we have to predict, we could also introduce a pseudo count\n",
    "and 0-probabilities are prevented.\n",
    " $$p(xi| L_k) = \\frac{1}{count(L_k) + |V| + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686b0ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gegen Audi-Chef Stadler wird wegen möglicher Verwicklungen in die Dieselaffäre ermittelt. Nun sitzt '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "english_text = nltk.corpus.gutenberg.raw(nltk.corpus.gutenberg.fileids())[:200000]\n",
    "def load_german_text(path):\n",
    "    text = \"\"\n",
    "    for f in pathlib.Path(path).glob(\"*.txt\"):\n",
    "        with open(f, \"r\") as openf:\n",
    "            text += openf.read()\n",
    "    return text\n",
    "german_text = load_german_text(\"../tagesschau_corpus/\")[:200000]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c00f8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def clean(s: str) -> str:\n",
    "    valid=string.ascii_letters+string.digits+\" \"\n",
    "    s = s.lower()\n",
    "    for c in valid:\n",
    "        s = \"\".join([c if c in valid else \" \" for c in s ])\n",
    "    s = re.sub(\"[\\s]+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf6e4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageClassifierBayes:\n",
    "    def __init__(self,texts, n=3, topk=10000):\n",
    "        self.n = n\n",
    "        counts = {k:nltk.FreqDist(self.create_ngrams(v)) for k, v in texts.items()}\n",
    "        \n",
    "        self.ngram_language_prob = {k:{k2:v2/sum(v.values()) for k2,v2 in v.most_common(topk)} for k,v in counts.items()}\n",
    "            \n",
    "        \n",
    "    def create_ngrams(self, texts: dict):\n",
    "        return [\"\".join(g) for w in nltk.tokenize.word_tokenize(texts) for g in nltk.ngrams(w, self.n)]\n",
    "    \n",
    "    def classify(self,text, prior = None):\n",
    "        d =  self.create_ngrams(clean(text))\n",
    "        p =  {language: [probs.get(ngram,1/len(probs)) for ngram in d] for language, probs in self.ngram_language_prob.items()}\n",
    "        p = {lang: reduce(lambda x, y: x*y, probs) for lang, probs in p.items()}\n",
    "        \n",
    "        if prior is None:\n",
    "            prior = [1/len(self.ngram_language_prob)] * len(self.ngram_language_prob)\n",
    "        \n",
    "        # Sort list by frequency\n",
    "        p = {lang: probs*pr for  (lang,probs), pr in zip(p.items(), prior)}\n",
    "        \n",
    "        sorted_counts = sorted(list(p.items()), key=lambda x: -x[1])\n",
    "\n",
    "        # Predict the language with most hits\n",
    "        return sorted_counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ec4931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NGLC = NGramLanguageClassifierBayes({\"english\":english_text, \"german\":german_text}, n=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d721d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

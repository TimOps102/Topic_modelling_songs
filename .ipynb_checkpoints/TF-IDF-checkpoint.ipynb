{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c881a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md',disable = ['parser','ner'])\n",
    "nlp.max_length = 3100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcb5b631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['keyword/brd_sentences.txt', 'keyword/ddr_sentences.txt', 'keyword/news.txt'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = {}\n",
    "with zipfile.ZipFile(\"keywords.zip\") as zfile:\n",
    "    for f in zfile.namelist():\n",
    "        if f != \"keyword/\":\n",
    "            content[f] = zfile.read(f).decode(\"utf8\")\n",
    "content.keys()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ce89fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    doc = nlp(txt)\n",
    "    words = [x.lemma_ if not x.is_punct else str(x) for x in doc]\n",
    "    words = [x.lemma_ for x in doc if x.pos_ in allowed_postags]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in punctuation]\n",
    "    words = [word for word in words if len(word)>2]\n",
    "    count = len(words)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "208a0bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    def create_dtm(self, texts, cut_first=1, min_freq=3):\n",
    "        \n",
    "        # Clean texts\n",
    "        self.texts = {k:preprocess(v) for k,v in texts.items()}\n",
    "        \n",
    "        # Count texts\n",
    "        self.vocab = nltk.FreqDist(sum(self.texts.values(),[]))\n",
    "        \n",
    "        # Prune overall vocabulary\n",
    "        self.vocab = sorted(list(self.vocab.items()),key= lambda x: -x[1]) \n",
    "        self.vocab = [x[0] for x in self.vocab[cut_first:] if x[1] >=min_freq]\n",
    "        \n",
    "        # count and restrict domain level text to the vocabulary\n",
    "        self.term_frequencies = {genre: nltk.FreqDist(text) for genre, text in self.texts.items()}\n",
    "        self.dtm = np.array([[v.get(w,0) for w in self.vocab] for k,v in self.term_frequencies.items()])\n",
    "    \n",
    "    def tfidf(self):\n",
    "        tf = np.log(self.dtm + 1e-25)\n",
    "        #tf = 0.5 + 0.5 * self.dtm / self.dtm.max(-1, keepdims=True) # alternative normalization.\n",
    "        idf = np.log(self.dtm.shape[0] /((da.dtm>0).sum(0) + 1e-25))\n",
    "        return tf * idf\n",
    "\n",
    "    def tfidf_keywords(self, n=10):\n",
    "        \"\"\"Iterate all copora for printing\"\"\"\n",
    "        tfidf = self.tfidf()\n",
    "        return {k:[self.vocab[k] for k in tfidf[i].argsort(0)[::-1][:n]] for i,k in enumerate(self.texts.keys())}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b111d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = TFIDF()\n",
    "da.create_dtm(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ef5d799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keyword/brd_sentences.txt': ['bum',\n",
       "  'Bum',\n",
       "  'mi',\n",
       "  'ho',\n",
       "  'aha',\n",
       "  'Tuut',\n",
       "  'dub',\n",
       "  'Amadeus',\n",
       "  'do',\n",
       "  'Hadschi',\n",
       "  'Annabelle',\n",
       "  'san',\n",
       "  'Tarzan',\n",
       "  'Goodbye',\n",
       "  'Fütter',\n",
       "  'Knutschfleck',\n",
       "  'Morgana',\n",
       "  'Fata',\n",
       "  'amore',\n",
       "  'Traumboy',\n",
       "  'nana-nana',\n",
       "  'Gianna',\n",
       "  'Mamy',\n",
       "  'mio',\n",
       "  'dat'],\n",
       " 'keyword/ddr_sentences.txt': [\"seh'n\",\n",
       "  \"geh'n\",\n",
       "  '`',\n",
       "  \"steh'n\",\n",
       "  'Kurschatt',\n",
       "  \"ander'n\",\n",
       "  \"uns're\",\n",
       "  'Elfenbein',\n",
       "  \"versteh'n\",\n",
       "  'Traumzeit',\n",
       "  \"and're\",\n",
       "  'Superfrau',\n",
       "  'On',\n",
       "  'Drache',\n",
       "  \"wenn's\",\n",
       "  'Nebelmeer',\n",
       "  'Fühlst',\n",
       "  \"zieh'n\",\n",
       "  \"geseh'n\",\n",
       "  'Susan',\n",
       "  'Lebensroulette',\n",
       "  \"bist'n\",\n",
       "  'Weltenmeer',\n",
       "  'n.',\n",
       "  \"über's\"],\n",
       " 'keyword/news.txt': ['\\t',\n",
       "  'Euro',\n",
       "  'Unternehmen',\n",
       "  'Trump',\n",
       "  '2018',\n",
       "  'politisch',\n",
       "  'Team',\n",
       "  'mehrere',\n",
       "  'Regierung',\n",
       "  'pro',\n",
       "  '2019',\n",
       "  'europäisch',\n",
       "  'Aktie',\n",
       "  'EU',\n",
       "  'zunächst',\n",
       "  '2017',\n",
       "  'ehemalig',\n",
       "  'FC',\n",
       "  'Angabe',\n",
       "  'übernehmen',\n",
       "  'Saison',\n",
       "  'betonen',\n",
       "  'bislang',\n",
       "  'entsprechend',\n",
       "  '2020']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.tfidf_keywords(n=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "songs",
   "language": "python",
   "name": "songs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

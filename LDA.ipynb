{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3a04ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e01a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51b7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "frg_path= \"Songs/BRD-Charts/1_xml/\"\n",
    "gdr_path= \"Songs/DDR/1_xml/\"\n",
    "ndw_path= \"Songs/NDW/1_xml/\"\n",
    "\n",
    "xml_frg = [frg_path+f for f in os.listdir(frg_path) if f.endswith('.xml')]\n",
    "xml_gdr = [gdr_path+f for f in os.listdir(gdr_path) if f.endswith('.xml')]\n",
    "xml_ndw = [ndw_path+f for f in os.listdir(ndw_path) if f.endswith('.xml')]\n",
    "\n",
    "all_xml = [xml_frg,xml_gdr,xml_ndw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35973acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for category in all_xml:\n",
    "    for xml_file in category:\n",
    "        with open(xml_file, 'r', encoding='utf-8') as file:\n",
    "            xml_data = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(xml_data, 'xml')\n",
    "        \n",
    "        cat = xml_file.split(\"/\")[1].split(\"-\")[0]\n",
    "        year = int(soup.find(\"date\").text)\n",
    "        title = soup.find(\"title\").text\n",
    "        author = soup.find(\"author\").text\n",
    "        text = \" \".join([k.text for k in soup.find(\"div1\", attrs={\"type\":\"song\"}).find_all(\"l\")])\n",
    "\n",
    "        data.append({'Category': cat, 'Year': year, 'Title': title, 'Author': author, 'Text': text})\n",
    "        \n",
    "df = pd.DataFrame(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df2121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndw_filter = (df[\"Category\"]==\"NDW\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "ndw_sub = df[ndw_filter].sample(n=202, random_state=42)\n",
    "brd_filter = (df[\"Category\"]==\"BRD\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "brd_sub = pd.concat([df[brd_filter],ndw_sub], ignore_index=True)\n",
    "ddr_filter = (df[\"Category\"]==\"DDR\") & (df['Year'] >= 1970) & (df['Year'] <= 1990)\n",
    "df = pd.concat([df[ddr_filter],brd_sub], ignore_index=True)\n",
    "\n",
    "#we now rename the NDW category to BRD, because the belong to the BRD\n",
    "df['Category'] = df['Category'].replace('NDW', 'BRD')\n",
    "#the dataframe that remains, contains now out of 1000 Songs of the GDR from 1970 to 1990 and 1000 Songs from the FRG from 1970 to 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fcfbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kennst', 'du', 'das', 'land', 'mit', 'seinen', 'alten', 'eichen', 'das', 'land', 'von', 'einstein', 'von', 'karl', 'marx', 'und', 'bach', 'wo', 'jede', 'antwort', 'endet', 'mit', 'dem', 'fragezeichen', 'wo', 'ich', 'ein', 'zimmer', 'habe', 'unterm', 'dach', 'wo', 'sich', 'so', 'viele', 'wegen', 'früher', 'oft', 'noch', 'schämen', 'wo', 'mancher', 'vater', 'eine', 'frage', 'nicht', 'versteht', 'wo', 'ihre', 'kinder', 'ihnen', 'das', 'nicht', 'übelnehmen', 'weil', 'seine', 'antwort', 'im', 'geschichtsbuch', 'steht', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'mit', 'seinen', 'seen', 'und', 'wäldern', 'das', 'kleine', 'land', 'das', 'man', 'an', 'einem', 'tag', 'durchfährt', 'wo', 'man', 'was', 'wird', 'auch', 'ohne', 'seine', 'eltern', 'doch', 'auch', 'beziehungen', 'sind', 'manchmal', 'etwas', 'wert', 'hier', 'steht', 'die', 'schule', 'und', 'mein', 'klassenzimmer', 'das', 'riecht', 'heut', 'immer', 'noch', 'nach', 'terpentin', 'von', 'mathe', 'hab', 'ich', 'heut', 'noch', 'keinen', 'schimmer', 'doch', 'vor', 'den', 'lehrern', 'kann', 'ich', 'meine', 'mütze', 'ziehn', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'wo', 'die', 'fabriken', 'uns', 'gehören', 'wo', 'der', 'prometheus', 'schon', 'um', 'fünf', 'aufsteht', 'hier', 'kann', 'man', 'manche', 'faust', 'auf', 'manchen', 'tischen', 'hören', 'bevor', 'dann', 'wieder', 'trotzdem', 'was', 'nicht', 'geht', 'wo', 'sich', 'auf', 'wohnungsämtern', 'hoffnungen', 'verlieren', 'wo', 'ein', 'parteitag', 'sich', 'darüber', 'sorgen', 'macht', 'wo', 'sich', 'die', 'leute', 'alles', 'selber', 'reparieren', 'weil', 'sie', 'das', 'werkzeug', 'haben', 'wissen', 'und', 'die', 'macht', 'hier', 'schaff', 'ich', 'selber', 'was', 'ich', 'einmal', 'werde', 'hier', 'geb', 'ich', 'meinem', 'leben', 'einen', 'sinn', 'hier', 'hab', 'ich', 'meinen', 'teil', 'von', 'unsrer', 'erde', 'der', 'kann', 'so', 'werden', 'wie', 'ich', 'selber', 'bin', 'das', 'ist', 'das', 'land', 'mit', 'dem', 'problem', 'im', 'winter', 'das', 'züge', 'stoppt', 'und', 'an', 'die', 'fenster', 'klirrt', 'wo', 'wir', 'viel', 'reden', 'über', 'später', 'und', 'auch', 'kinder', 'und', 'wo', 'ein', 'cello', 'spielt', 'bevor', 'es', 'leise', 'wird', 'hier', 'lernte', 'meine', 'mutter', 'das', 'regieren', 'als', 'sie', 'vor', 'einem', 'trümmerhaufen', 'stand', 'ich', 'möchte', 'dieses', 'land', 'nie', 'mehr', 'verlieren', 'es', 'ist', 'mein', 'mutter', 'und', 'mein', 'vaterland']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        yield(gensim.utils.simple_preprocess(str(sentence)))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df[\"Text\"]))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65727d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abends', 'wartest', 'du', 'ganz', 'zufällig', 'am', 'bus', 'du', 'weißt', 'genau', 'zu', 'welcher', 'zeit', 'ich', 'kommen', 'muss', 'du', 'legst', 'dein', 'lächeln', 'auf', 'und', 'tust', 'immer', 'vertraut', 'dass', 'jeder', 'denken', 'muss', 'ich', 'wäre', 'deine', 'braut', 'deine', 'braut', 'abends', 'kommst', 'du', 'an', 'und', 'lädst', 'dich', 'selber', 'ein', 'und', 'immer', 'mit', 'der', 'flasche', 'wein', 'sprichst', 'große', 'dinge', 'und', 'machst', 'es', 'dir', 'bequem', 'und', 'kannst', 'dann', 'gar', 'nicht', 'lachen', 'wenn', 'ich', 'dann', 'sage', 'du', 'sollst', 'geh', 'du', 'sollst', 'geh', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'vor', 'deinen', 'freunden', 'drehst', 'du', 'und', 'spielst', 'den', 'king', 'als', 'wär', 'ich', 'nur', 'ein', 'kleiner', 'fisch', 'der', 'an', 'dir', 'hing', 'du', 'kippst', 'die', 'sache', 'um', 'dass', 'du', 'dich', 'nicht', 'blamierst', 'ich', 'trinke', 'deinen', 'wein', 'und', 'merke', 'wie', 'du', 'frierst', 'wie', 'du', 'frierst', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett', 'hohoho', 'machst', 'du', 'auch', 'höflich', 'und', 'nett', 'hohoho', 'alles', 'routine', 'ich', 'hab', 'dich', 'erkannt', 'du', 'kommst', 'nicht', 'in', 'mein', 'du', 'kommst', 'nicht', 'in', 'mein', 'du', 'kommst', 'nicht', 'in', 'mein', 'bett']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(bigram_mod[data_words[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "207fe892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_.lower() for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1407fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['land', 'einstein', 'antwort', 'fragezeichen', 'dach', 'vater', 'frage', 'kind', 'antwort', 'geschichtsbuch', 'schaff', 'geb', 'sinn', 'teil', 'erde', 'seen', 'land', 'tag', 'eltern', 'beziehung', 'klassenzimmer', 'terpentin', 'lehrer', 'mütze', 'schaff', 'geb', 'sinn', 'teil', 'erde', 'land', 'fabrik', 'wohnungsämtern', 'hoffnung', 'parteitag', 'leute', 'werkzeug', 'schaff', 'geb', 'sinn', 'teil', 'erde', 'land', 'problem', 'züge', 'fenster', 'kind', 'mutter', 'trümmerhaufe', 'land', 'mutter']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load('de_core_news_md', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45b52eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 7084\n",
      "Total Vocabulary Size: 906\n",
      "[[(0, 2), (1, 1), (2, 1), (3, 3), (4, 1), (5, 1), (6, 1), (7, 3), (8, 1), (9, 2), (10, 5), (11, 1), (12, 1), (13, 2), (14, 1), (15, 3), (16, 3), (17, 1), (18, 3), (19, 1), (20, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "print('Total Vocabulary Size:', len(id2word))\n",
    "id2word.filter_extremes(no_below=5)\n",
    "print('Total Vocabulary Size:', len(id2word))\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43f25235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=1000,\n",
    "                                           passes=100,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20ba3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.145*\"lied\" + 0.072*\"komm\" + 0.036*\"rock_roll\" + 0.030*\"frieden\" + '\n",
      "  '0.030*\"band\" + 0.027*\"erde\" + 0.019*\"ende\" + 0.017*\"schritt\" + '\n",
      "  '0.017*\"mensch\" + 0.016*\"krieg\"'),\n",
      " (1,\n",
      "  '0.147*\"auge\" + 0.073*\"brauch\" + 0.071*\"tür\" + 0.038*\"papa\" + '\n",
      "  '0.030*\"tanz_tanz\" + 0.025*\"glas\" + 0.025*\"gefühl\" + 0.024*\"haar\" + '\n",
      "  '0.024*\"stern\" + 0.022*\"stück\"'),\n",
      " (2,\n",
      "  '0.097*\"land\" + 0.093*\"wind\" + 0.067*\"stein\" + 0.047*\"stadt\" + 0.045*\"sand\" '\n",
      "  '+ 0.043*\"name\" + 0.043*\"tod\" + 0.037*\"bild\" + 0.028*\"mensch\" + '\n",
      "  '0.019*\"erinnerung\"'),\n",
      " (3,\n",
      "  '0.119*\"kind\" + 0.113*\"tag\" + 0.092*\"haus\" + 0.035*\"welt\" + 0.022*\"wasser\" + '\n",
      "  '0.017*\"blick\" + 0.013*\"bescheid\" + 0.013*\"berg\" + 0.012*\"auge\" + '\n",
      "  '0.012*\"schiff\"'),\n",
      " (4,\n",
      "  '0.146*\"mädchen\" + 0.089*\"mensch\" + 0.064*\"wort\" + 0.026*\"mama\" + '\n",
      "  '0.017*\"gefühl\" + 0.016*\"versteh\" + 0.016*\"sitz\" + 0.016*\"stunde\" + '\n",
      "  '0.016*\"tag\" + 0.014*\"bier\"'),\n",
      " (5,\n",
      "  '0.078*\"lust\" + 0.066*\"fühl\" + 0.061*\"seele\" + 0.049*\"schnee\" + 0.037*\"fall\" '\n",
      "  '+ 0.036*\"wärme\" + 0.031*\"eis\" + 0.024*\"abschied\" + 0.024*\"ah_ah\" + '\n",
      "  '0.022*\"tier\"'),\n",
      " (6,\n",
      "  '0.209*\"liebe\" + 0.149*\"welt\" + 0.115*\"glück\" + 0.084*\"leben\" + '\n",
      "  '0.032*\"mensch\" + 0.028*\"träne\" + 0.023*\"wort\" + 0.019*\"blume\" + '\n",
      "  '0.019*\"spiel\" + 0.016*\"wunder\"'),\n",
      " (7,\n",
      "  '0.083*\"traum\" + 0.058*\"träume\" + 0.049*\"feuer\" + 0.040*\"erde\" + '\n",
      "  '0.040*\"welt\" + 0.034*\"licht\" + 0.027*\"tag\" + 0.023*\"schatten\" + '\n",
      "  '0.022*\"glaub\" + 0.019*\"gesicht\"'),\n",
      " (8,\n",
      "  '0.299*\"zeit\" + 0.082*\"jahr\" + 0.041*\"sehnsucht\" + 0.037*\"tag\" + '\n",
      "  '0.030*\"sinn\" + 0.029*\"ende\" + 0.028*\"zug\" + 0.020*\"ziel\" + 0.019*\"wort\" + '\n",
      "  '0.019*\"frage\"'),\n",
      " (9,\n",
      "  '0.303*\"nacht\" + 0.065*\"tag\" + 0.046*\"spaß\" + 0.041*\"mond\" + 0.027*\"stadt\" + '\n",
      "  '0.018*\"arbeit\" + 0.016*\"zimmer\" + 0.015*\"schatz\" + 0.014*\"hand\" + '\n",
      "  '0.011*\"fenster\"'),\n",
      " (10,\n",
      "  '0.090*\"straße\" + 0.076*\"oh\" + 0.047*\"himmel\" + 0.043*\"angst\" + '\n",
      "  '0.037*\"sonnenschein\" + 0.036*\"blut\" + 0.026*\"luft\" + 0.025*\"welt\" + '\n",
      "  '0.021*\"zeichen\" + 0.021*\"zukunft\"'),\n",
      " (11,\n",
      "  '0.262*\"mann\" + 0.146*\"frau\" + 0.037*\"vater\" + 0.035*\"herr\" + 0.020*\"hause\" '\n",
      "  '+ 0.019*\"haus\" + 0.017*\"ohr\" + 0.016*\"jahr\" + 0.013*\"krieg\" + '\n",
      "  '0.013*\"gesicht\"'),\n",
      " (12,\n",
      "  '0.127*\"freund\" + 0.064*\"musik\" + 0.050*\"bett\" + 0.050*\"hand\" + 0.038*\"kopf\" '\n",
      "  '+ 0.027*\"spiel\" + 0.023*\"sohn\" + 0.022*\"platz\" + 0.016*\"wahl\" + '\n",
      "  '0.015*\"tut_weh\"'),\n",
      " (13,\n",
      "  '0.220*\"herz\" + 0.175*\"la_la\" + 0.063*\"tanz\" + 0.037*\"lied\" + 0.024*\"lieder\" '\n",
      "  '+ 0.024*\"mund\" + 0.018*\"singen\" + 0.017*\"welt\" + 0.015*\"sehnsucht\" + '\n",
      "  '0.012*\"tor\"'),\n",
      " (14,\n",
      "  '0.187*\"sonne\" + 0.061*\"geld\" + 0.057*\"sommer\" + 0.040*\"welt\" + '\n",
      "  '0.037*\"sonntag\" + 0.031*\"boot\" + 0.029*\"wolke\" + 0.025*\"freiheit\" + '\n",
      "  '0.024*\"chance\" + 0.023*\"doktor\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "songs",
   "language": "python",
   "name": "songs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
